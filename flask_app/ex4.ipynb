{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lxml.html\n",
    "import lxml.etree\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from sqlalchemy import func, create_engine\n",
    "from sqlalchemy import Column, Integer, Text, DateTime\n",
    "from sqlalchemy.schema import Index\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.declarative import declarative_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "class News(Base):\n",
    "    __tablename__ = 'news'\n",
    "\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    body = Column(Text, nullable=False)\n",
    "    category = Column(Text, nullable=False)\n",
    "    \n",
    "    def __init__(self, body, category):\n",
    "        self.body = body\n",
    "        self.category = category\n",
    "\n",
    "\n",
    "def init_db(db_url):\n",
    "    engine = create_engine(db_url)\n",
    "    Base.metadata.bind = engine\n",
    "    Base.metadata.create_all()\n",
    "    return sessionmaker(bind=engine)\n",
    "\n",
    "db_session = init_db('sqlite:///lenta.db')()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def followTheLink(link):\n",
    "    url = 'https://www.lenta.ru' + link\n",
    "    response = requests.get(url)\n",
    "    return lxml.html.fromstring(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processItem(tree):\n",
    "    category = tree.xpath('//a[@class=\"b-header-inner__block\"]/text()')[0]\n",
    "    body = ' '.join(tree.xpath('//div[@itemprop=\"articleBody\"]/p/text()')).replace(u'\\xa0', u' ')\n",
    "    return category, body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "now_url = '/{}/{}/{}'.format(now.year, now.month, now.day)\n",
    "tree = followTheLink(now_url) #начинаем с архива за сегодняшнее число"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "now_url = '/{}/{}/{}'.format('2014', '04', '15')\n",
    "tree = followTheLink(now_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = []\n",
    "day = 0\n",
    "while '2012' not in date: #переходим на предыдущее число, пока не встретим 2014\n",
    "    date = tree.xpath('//div[@class=\"b-archive__header-rubric\"]/h1/span/text()')[0].split()\n",
    "    day += 1\n",
    "    print(day \n",
    "    item_urles = tree.xpath('//section[@class=\"b-longgrid-column\"]/div/div[2]/h3/a/@href')\n",
    "    \n",
    "    for url in item_urles:\n",
    "        itemTree = followTheLink(url)\n",
    "        category, body = processItem(itemTree)\n",
    "        news = News(body, category)\n",
    "        db_session.add(news)\n",
    "    db_session.commit()\n",
    "    next_page_url = tree.xpath('//a[@class=\"control_mini\"]/@href')[0]\n",
    "    followTheLink(next_page_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#db_session.query(News).filter(News.category == '69-я параллель').delete()\n",
    "#db_session.query(News).filter(News.category == 'Мир').delete()\n",
    "db_session.query(News).filter(News.category == 'Бывший СССР', News.id > 146603).delete()\n",
    "db_session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_id = [instance.id for instance in db_session.query(News).filter(News.category == 'Бывший СССР').order_by(News.id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146603"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_id[15000]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Наука и техника': 15001, 'Экономика': 15001, 'Россия': 15001, 'Культура': 10385, 'Спорт': 15001, 'Ценности': 8206, 'Из жизни': 14899, 'Интернет и СМИ': 15001, 'Силовые структуры': 15001, 'Бывший СССР': 15001}\n"
     ]
    }
   ],
   "source": [
    "cats = dict()\n",
    "for instance in db_session.query(News).order_by(News.id):\n",
    "    if instance.category in cats:\n",
    "        cats[instance.category] += 1\n",
    "    else:\n",
    "        cats[instance.category] = 1\n",
    "            \n",
    "print(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Наука и техника': 0, 'Экономика': 1, 'Россия': 2, 'Культура': 3, 'Спорт': 4, 'Ценности': 5, 'Из жизни': 6, 'Интернет и СМИ': 7, 'Силовые структуры': 8, 'Бывший СССР': 9}\n"
     ]
    }
   ],
   "source": [
    "categories = dict()\n",
    "for i, cat in enumerate(db_session.query(News.category).distinct()):\n",
    "    categories[cat[0]] = i\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "\n",
    "#pipeline = Pipeline([\n",
    "#    ('vect', CountVectorizer(max_df=0.2, ngram_range=(1,2))),\n",
    "#    ('tfidf', TfidfTransformer()),\n",
    "#    ('clf', SGDClassifier(penalty='elasticnet')),\n",
    "#])\n",
    "text_clf = Pipeline([('vectorizer', CountVectorizer(max_df=0.2, ngram_range=(1,2))),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', SGDClassifier(penalty='elasticnet'))])\n",
    "\n",
    "text_clf = text_clf.fit([instance.body for instance in db_session.query(News).order_by(News.id)],\n",
    "                       [categories[i.category] for i in db_session.query(News).order_by(News.id)])\n",
    "\n",
    "\n",
    "#vectorizer = CountVectorizer()\n",
    "#tfidf_transformer = TfidfTransformer()\n",
    "#x_vect = vectorizer.fit_transform([instance.body for instance in db_session.query(News).order_by(News.id)])\n",
    "#x_tfidf = tfidf_transformer.fit_transform(x_vect)\n",
    "#num_cats = [categories[instance.category] for instance in db_session.query(News).order_by(News.id)]\n",
    "#clf = SGDClassifier().fit(x_tfidf, num_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 4 3 4 1]\n"
     ]
    }
   ],
   "source": [
    "data = ['нейтрон', \n",
    "        'Сенат США поддержал предложенный администрацией президента Дональда Трампа план налоговой реформы. Теперь два похожих, но немного отличающихся варианта законопроекта должны быть согласованы и приняты обеими палатами, после чего отправлены на подпись главе государства. Реформа является беспрецедентной по масштабам со времен Рональда Рейгана, но скептики считают, что она может все испортить. В частности, звучат опасения по поводу раздувания бюджетного дефицита, а также из-за роста и без того значительной разницы в доходах бедных и богатых.',  \n",
    "        'Грабитель ворвался в магазин и, угрожая продавщице пистолетом, потребовал открыть кассу. В это время мимо торговой точки проходила женщина, и у преступника при виде свидетеля сдали нервы — он разбил витрину, схватил товар и выбежал на улицу.', \n",
    "        'Блогер из Британии, известный в сети под ником jamielliottg, поссорил два голосовых помощника Amazon Echo. Ролик опубликован на его YouTube-канале.', \n",
    "        'Картина «Первому игроку приготовиться» выйдет на экраны 28 марта 2018 года. Она основана на одноименном научно-фантастическом романе Эрнеста Клайна. По сюжету, в недалеком будущем мировая экономика оказалась в критическом положении из-за дефицита ресурсов, и люди находят спасение в иммерсивной онлайн-игре, в которую можно погружаться при помощи VR-шлема и костюма.', \n",
    "        'футбол',\n",
    "        'Свадебный наряд состоит из топа с длинными рукавами и фестонами, застегивающегося на молнию по примеру спортивной куртки, и белых тренировочных штанов с широким передником с оборками из нескольких слоев полупрозрачного газа. Костюм дополняет фата. Создатель необычного наряда — креативный директор марки Vetements модельер Демна Гвасалия.']\n",
    "\n",
    "print(text_clf.predict(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'vect__max_df': (0.5, 0.75, 1.0), 'clf__penalty': ('l2', 'elasticnet')}\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:  3.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 265.122s\n",
      "\n",
      "Best score: 0.943\n",
      "Best parameters set:\n",
      "\tclf__penalty: 'elasticnet'\n",
      "\tvect__max_df: 0.5\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    #'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    #'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    # 'clf__n_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "print(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit([instance.body for instance in db_session.query(News).order_by(News.id)], \n",
    "                [categories[i.category] for i in db_session.query(News).order_by(News.id)])\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
